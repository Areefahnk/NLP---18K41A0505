{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNyDM54r94iY9yhCPUMQPIl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Areefahnk/NLP---18K41A0505/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTsaNYUSMvxG"
      },
      "source": [
        "**NAME: AREEFA\n",
        "ROLL NO: 18K41A0505**\n",
        "\n",
        "CSE IV SREC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VqgS8Xv392v"
      },
      "source": [
        "Are you fascinated by the amount of text data available on the internet? Are you looking for ways to work with this text data but aren’t sure where to begin? Machines, after all, recognize numbers, not the letters of our language. And that can be a tricky landscape to navigate in machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTN3-zZd4D8E"
      },
      "source": [
        "Solving an NLP problem is a multi-stage process. We need to clean the unstructured text data first before we can even think about getting to the modeling stage. Cleaning the data consists of a few key steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKonTB7G4CAV",
        "outputId": "2bb916a7-491d-4a6d-bf31-a7f142a13bf0"
      },
      "source": [
        "pip install --user -U nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.62.3)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2021.10.23-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\n",
            "\u001b[K     |████████████████████████████████| 748 kB 58.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Installing collected packages: regex, nltk\n",
            "\u001b[33m  WARNING: The script nltk is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed nltk-3.6.5 regex-2021.10.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7cOt8YJD9XY"
      },
      "source": [
        "text='''Are you fascinated by the amount of text data available on the internet? \n",
        "Are you looking for ways to work with this text data but aren’t sure where to begin? Machines, after all, recognize numbers, not the letters of our language. And that can be a tricky landscape to navigate in machine learning.'''"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2jdTHDNEhZG"
      },
      "source": [
        "**1. Split the above paragraph into sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byiBewY1E_C7",
        "outputId": "c4a50bad-9b0d-4896-a00a-77d75011a462"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfGkh67WFNsR",
        "outputId": "cdfa32f9-5049-4162-dbed-f25ee204dfa5"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(text)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Are you fascinated by the amount of text data available on the internet?',\n",
              " 'Are you looking for ways to work with this text data but aren’t sure where to begin?',\n",
              " 'Machines, after all, recognize numbers, not the letters of our language.',\n",
              " 'And that can be a tricky landscape to navigate in machine learning.']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yHUOQnuFmdh"
      },
      "source": [
        "**2.\tSplit the above paragraph into words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcMubkXNEcHM",
        "outputId": "666d673a-2f7e-4325-b6a3-20321ed1bd6e"
      },
      "source": [
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Are', 'you', 'fascinated', 'by', 'the', 'amount', 'of', 'text', 'data', 'available', 'on', 'the', 'internet', '?', 'Are', 'you', 'looking', 'for', 'ways', 'to', 'work', 'with', 'this', 'text', 'data', 'but', 'aren', '’', 't', 'sure', 'where', 'to', 'begin', '?', 'Machines', ',', 'after', 'all', ',', 'recognize', 'numbers', ',', 'not', 'the', 'letters', 'of', 'our', 'language', '.', 'And', 'that', 'can', 'be', 'a', 'tricky', 'landscape', 'to', 'navigate', 'in', 'machine', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03ONYBehFvyr"
      },
      "source": [
        "**3.\tFind stem and lemma words for the given words?**\n",
        "\n",
        "“cats\"\n",
        "\"trouble\"\n",
        "\"troubling\"\n",
        "\"troubled\"\n",
        "“having”\n",
        "“Corriendo”\n",
        "“at”\n",
        "“was”\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjLfxyUjHulG"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTdYwqZkErpL"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIideP4SI2yp"
      },
      "source": [
        "**PorterStemmer being the oldest one originally developed in 1979. LancasterStemmer was developed in 1990 and uses a more aggressive approach than Porter Stemming Algorithm.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIUc-9nIHpy6"
      },
      "source": [
        "**Using PorterStemmer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YZ8ZWUnHhRY",
        "outputId": "c878c0b6-02a9-4f4a-fd35-fe4cdd076d2e"
      },
      "source": [
        "porter = PorterStemmer()\n",
        "words=['cats','trouble','troubling','troubled','having','Corriendo','at','was']\n",
        "for i in words:\n",
        "  print(\"{} --> {}\".format(i,porter.stem(i)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cats --> cat\n",
            "trouble --> troubl\n",
            "troubling --> troubl\n",
            "troubled --> troubl\n",
            "having --> have\n",
            "Corriendo --> corriendo\n",
            "at --> at\n",
            "was --> wa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0TpIK_9Icac"
      },
      "source": [
        "**Using LancasterStemmer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYiR91b_IQuT",
        "outputId": "19be75e6-a71c-4b04-b8ab-8f04efa007f9"
      },
      "source": [
        "lancaster=LancasterStemmer()\n",
        "words=['cats','trouble','troubling','troubled','having','Corriendo','at','was']\n",
        "for i in words:\n",
        "  print(\"{} --> {}\".format(i,lancaster.stem(i)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cats --> cat\n",
            "trouble --> troubl\n",
            "troubling --> troubl\n",
            "troubled --> troubl\n",
            "having --> hav\n",
            "Corriendo --> corriendo\n",
            "at --> at\n",
            "was --> was\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBcxArf-JAEu"
      },
      "source": [
        "**Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O_gKDLcIsf3",
        "outputId": "a01d84e7-f89b-479c-9c6b-9caa47173666"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "words=['cats','trouble','troubling','troubled','having','Corriendo','at','was']\n",
        "for i in words:\n",
        "  print(\"{} --> {}\".format(i,wordnet_lemmatizer.lemmatize(i)))\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "cats --> cat\n",
            "trouble --> trouble\n",
            "troubling --> troubling\n",
            "troubled --> troubled\n",
            "having --> having\n",
            "Corriendo --> Corriendo\n",
            "at --> at\n",
            "was --> wa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DF7kuWpJpe2"
      },
      "source": [
        "**4.\tFind stop words from the given paragraph?**\n",
        "\n",
        "“The NLTK library is one of the oldest and most commonly used Python libraries for Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the corpus module. To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.”\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krGn-IxDLhzx",
        "outputId": "0bc82797-781e-4295-ae9e-0e834bd47ad7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmaaCdl5Jev2",
        "outputId": "e83ab997-d9f9-428c-9048-de9f631ab1b1"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        " \n",
        "para = '''The NLTK library is one of the oldest and most commonly used Python libraries for Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the corpus module. To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.'''\n",
        " \n",
        "stopwords = set(stopwords.words('english'))\n",
        " \n",
        "words = word_tokenize(para)\n",
        " \n",
        "res = [w for w in words if not w.lower() in stopwords]\n",
        " \n",
        "res = []\n",
        " \n",
        "for w in words:\n",
        "    if w not in stopwords:\n",
        "        res.append(w)\n",
        " \n",
        "#print(words)\n",
        "print(res)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'NLTK', 'library', 'one', 'oldest', 'commonly', 'used', 'Python', 'libraries', 'Natural', 'Language', 'Processing', '.', 'NLTK', 'supports', 'stop', 'word', 'removal', ',', 'find', 'list', 'stop', 'words', 'corpus', 'module', '.', 'To', 'remove', 'stop', 'words', 'sentence', ',', 'divide', 'text', 'words', 'remove', 'word', 'exits', 'list', 'stop', 'words', 'provided', 'NLTK', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wX2QB3GLt8J"
      },
      "source": [
        "**5.\tFrom the above paragraph print frequency of each word using NLTK?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VovzxzwKLdrN",
        "outputId": "0d02ae6f-6d6b-4e84-be4c-d1f1bcbad119"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import webtext\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "para = '''he NLTK library  is  one  of  the  oldest  and  most  commonly  used  Python  libraries  for \n",
        "Natural Language Processing. NLTK supports stop word removal, and you can find the list \n",
        "of stop words in the  corpus  module. To remove stop words from a sentence, you can divide \n",
        "your text into words and then remove the word if it exits in the list of stop words provided \n",
        "by NLTK.'''\n",
        " \n",
        " \n",
        "words = word_tokenize(para)\n",
        "data_analysis = nltk.FreqDist(words)\n",
        "\n",
        "filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 3])\n",
        " \n",
        "for key in sorted(filter_words):\n",
        "    print(\"%s: %s\" % (key, filter_words[key]))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language: 1\n",
            "NLTK: 3\n",
            "Natural: 1\n",
            "Processing: 1\n",
            "Python: 1\n",
            "commonly: 1\n",
            "corpus: 1\n",
            "divide: 1\n",
            "exits: 1\n",
            "find: 1\n",
            "from: 1\n",
            "into: 1\n",
            "libraries: 1\n",
            "library: 1\n",
            "list: 2\n",
            "module: 1\n",
            "most: 1\n",
            "oldest: 1\n",
            "provided: 1\n",
            "removal: 1\n",
            "remove: 2\n",
            "sentence: 1\n",
            "stop: 4\n",
            "supports: 1\n",
            "text: 1\n",
            "then: 1\n",
            "used: 1\n",
            "word: 2\n",
            "words: 4\n",
            "your: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXjEF_ddMXEB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}